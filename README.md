# ğŸ¤– AI MERP - Advanced Conversational AI Chatbot

> A modern AI chatbot powered by **real language models** (Ollama), not templates. Runs completely locally and free.

## âœ¨ Features

- ğŸ§  **Real AI Understanding** - Powered by Mistral 7B or other open-source models
- ğŸ’¬ **Genuine Conversations** - Context-aware responses that actually make sense
- ğŸ“š **Learning System** - Remembers your name, interests, and emotional state
- ğŸ¨ **Modern UI** - Glass morphism dark theme with real-time status
- ğŸ”’ **100% Local** - No cloud, no tracking, completely private
- âš¡ **GPU Accelerated** - Uses your RTX 3060 for fast inference
- ğŸ†“ **Completely Free** - No subscriptions or API costs

## ğŸš€ Quick Start

### Prerequisites
- Windows/Mac/Linux with Python 3.9+
- 8GB RAM minimum (16GB recommended for better models)
- NVIDIA GPU optional but recommended (you have RTX 3060 âœ“)

### Step 1: Install Ollama
```bash
# Download from https://ollama.ai
# Install and run the installer
```

### Step 2: Download a Model
```bash
ollama pull mistral
# or: ollama pull neural-chat
```

### Step 3: Start Ollama (keep running in background)
```bash
ollama serve
```

### Step 4: Run Chatbot (in new terminal)
```bash
cd ai-project
.\venv\Scripts\activate
python main.py
```

## ğŸ“Š Example Conversations

### Before (Pattern Matching)
```
You: my girl cheated on me
Bot: New recipe or restaurant?  âŒ WRONG CONTEXT
```

### After (Real LLM)
```
You: my girl cheated on me
Bot: I'm really sorry to hear that. Betrayal is incredibly painful. 
     How are you holding up? Do you have people to talk to about this?  âœ… EMPATHETIC & CONTEXTUAL
```

## ğŸ¯ Available Models

```bash
ollama pull mistral          # â­ Recommended - Fast & Quality (5GB)
ollama pull neural-chat      # Good for conversations (5GB)
ollama pull llama2           # Meta's model (4GB)
ollama pull dolphin-mixtral  # Most capable but slow (26GB)
```

## ğŸ”§ Usage

### Change Model
Edit `main.py` or the ChatbotApp initialization:
```python
self.model = LLMChatbotModel(model_name="neural-chat")
```

### Adjust Response Style
In `src/ollama_model.py`, modify `temperature`:
```python
"temperature": 0.7,  # 0.1 = precise, 1.0 = creative
```

## ğŸ“ Project Structure
```
ai-project/
â”œâ”€â”€ main.py                 # Entry point
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ OLLAMA_SETUP.md        # Detailed setup guide
â”œâ”€â”€ run_chatbot.bat        # Quick start script
â””â”€â”€ src/
    â”œâ”€â”€ gui.py             # Modern UI
    â”œâ”€â”€ ollama_model.py    # LLM chatbot engine
    â””â”€â”€ merp_model.py      # Legacy pattern-matching (kept for reference)
```

## ğŸ› Troubleshooting

| Issue | Solution |
|-------|----------|
| "Ollama not connected" | Make sure `ollama serve` is running |
| Model not found | Run `ollama pull mistral` |
| Slow responses | Normal first run - GPU is loading model |
| Out of memory | Close other apps, try smaller model |

## ğŸ“– Full Setup Guide

See [OLLAMA_SETUP.md](OLLAMA_SETUP.md) for detailed instructions and advanced configuration.

## ğŸ“ Learning More

- [Ollama Docs](https://github.com/ollama/ollama)
- [Model Library](https://ollama.ai/library)
- [Mistral 7B](https://mistral.ai)
- [How Language Models Work](https://karpathy.ai)

## ğŸ“ What's Inside

### Core Features
- Real conversational AI using Ollama
- Context-aware responses
- User learning and preference tracking
- Conversation history
- Modern glass morphism UI with Tkinter

### How It Works
1. User sends message
2. Chat history + user preferences are sent to LLM
3. Ollama generates contextual response using GPU
4. Response appears in UI with proper formatting
5. Interaction is logged for learning

## ğŸ” Privacy
100% local processing - nothing leaves your computer. All data stays on your machine.

## ğŸ“„ License
Open source - modify and use as you wish!

---

**Ready to chat with real AI? ğŸš€**

Make sure Ollama is running, then launch the chatbot!
